{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13965568,"sourceType":"datasetVersion","datasetId":8902634},{"sourceId":13969096,"sourceType":"datasetVersion","datasetId":8905183},{"sourceId":14030877,"sourceType":"datasetVersion","datasetId":8934740},{"sourceId":14034747,"sourceType":"datasetVersion","datasetId":8937311},{"sourceId":14108008,"sourceType":"datasetVersion","datasetId":8976080}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n!pip install biopython obonet --quiet\n!pip install transformers biopython --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:38:54.178791Z","iopub.execute_input":"2025-12-04T03:38:54.179113Z","iopub.status.idle":"2025-12-04T03:39:06.777832Z","shell.execute_reply.started":"2025-12-04T03:38:54.179081Z","shell.execute_reply":"2025-12-04T03:39:06.776315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import EsmTokenizer, EsmModel\nimport re\nfrom Bio import SeqIO\nfrom collections import defaultdict\n\n# --- 1.1. X·ª≠ l√Ω file Taxonomy ---\ndef parse_taxonomy(taxon_file_path):\n    \"\"\"T·∫°o √°nh x·∫° t·ª´ ID protein sang Taxon ID.\"\"\"\n    prot_to_taxon = {}\n    unique_taxa = set()\n    with open(taxon_file_path, 'r') as f:\n        for line in f:\n            parts = line.strip().split('\\t')\n            if len(parts) == 2:\n                protein_id = parts[0].strip()\n                taxon_id = parts[1].strip()\n                prot_to_taxon[protein_id] = taxon_id\n                unique_taxa.add(taxon_id)\n    return prot_to_taxon, sorted(list(unique_taxa))\n\n# --- 1.2. X·ª≠ l√Ω file Sequences (FASTA) ---\ndef parse_sequences(fasta_file_path):\n    \"\"\"T·∫°o √°nh x·∫° t·ª´ ID protein sang Chu·ªói axit amin.\"\"\"\n    prot_to_seq = {}\n    # S·ª≠ d·ª•ng SeqIO ƒë·ªÉ ƒë·ªçc file FASTA hi·ªáu qu·∫£\n    for record in SeqIO.parse(fasta_file_path, \"fasta\"):\n        # L·∫•y Uniprot ID t·ª´ header (v√≠ d·ª•: sp|A0A1D9BZF0|...)\n        uniprot_id_match = re.search(r'\\|([A-Z0-9]+)\\|', record.id)\n        if uniprot_id_match:\n            protein_id = uniprot_id_match.group(1)\n            prot_to_seq[protein_id] = str(record.seq)\n    return prot_to_seq\n\n# --- D·ªØ li·ªáu gi·∫£ ƒë·ªãnh ---\n# Trong th·ª±c t·∫ø, b·∫°n s·∫Ω thay th·∫ø b·∫±ng ƒë∆∞·ªùng d·∫´n file:\nprot_to_taxon, unique_taxa_from_func = parse_taxonomy(\"/kaggle/input/cafa56/CAFA56/CAFA56_train_taxonomy.tsv\")\nprot_to_seq = parse_sequences(\"/kaggle/input/cafa56/CAFA56/CAFA56_sequences.fasta\")\n\nunique_taxa = unique_taxa_from_func\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T06:04:12.555217Z","iopub.execute_input":"2025-12-03T06:04:12.555628Z","iopub.status.idle":"2025-12-03T06:04:43.728593Z","shell.execute_reply.started":"2025-12-03T06:04:12.555586Z","shell.execute_reply":"2025-12-03T06:04:43.727910Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# T·∫°o √°nh x·∫° t·ª´ Taxon ID (chu·ªói) sang Index (s·ªë nguy√™n)\ntaxon_to_index = {taxon: i for i, taxon in enumerate(unique_taxa)}\nnum_taxon = len(unique_taxa) # S·ªë l∆∞·ª£ng taxon duy nh·∫•t\nprint(num_taxon)\nprint(1)\n# V√≠ d·ª•: num_taxon = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T06:04:43.729313Z","iopub.execute_input":"2025-12-03T06:04:43.729862Z","iopub.status.idle":"2025-12-03T06:04:43.734742Z","shell.execute_reply.started":"2025-12-03T06:04:43.729842Z","shell.execute_reply":"2025-12-03T06:04:43.734044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# T·∫£i m√¥ h√¨nh ESM-2\nimport torch\nimport gc\nfrom tqdm.auto import tqdm\nimport time\n\n\ntorch.cuda.empty_cache()\ngc.collect()\n# --- Danh s√°ch top 10 taxa ---\ntop_taxa = [\"9606\", \"10090\", \"3702\", \"559292\", \"10116\", \"284812\", \n            \"83333\", \"7227\", \"6239\", \"83332\"]\ntaxon_to_index_top = {taxon: i for i, taxon in enumerate(top_taxa)}\nothers_index = len(top_taxa)        # index cho 'others'\nnum_taxon_top = len(top_taxa) + 1   # 11 chi·ªÅu\n\ndef prot_taxon_onehot(prot_id, prot_to_taxon, num_taxon_top=num_taxon_top, taxon_to_index_top=taxon_to_index_top):\n    \"\"\"\n    Nh·∫≠n protein ID, tr·∫£ v·ªÅ vector one-hot 11 chi·ªÅu cho Taxon\n    \"\"\"\n    taxon_id_str = prot_to_taxon.get(prot_id, None)\n    if taxon_id_str is None:\n        # N·∫øu kh√¥ng t√¨m th·∫•y taxon -> g√°n v√†o 'others'\n        index = others_index\n    else:\n        index = taxon_to_index_top.get(taxon_id_str, others_index)\n    \n    vec = torch.zeros(num_taxon_top, dtype=torch.float32)\n    vec[index] = 1\n    return vec\n\n\nmodel_name = \"facebook/esm2_t33_650M_UR50D\"\ntokenizer = EsmTokenizer.from_pretrained(model_name)\nmodel_esm2 = EsmModel.from_pretrained(model_name)\n\n# ESM-2 c√≥ k√≠ch th∆∞·ªõc embedding\nEMBEDDING_DIM = model_esm2.config.hidden_size \nprint(f\"Embedding dimension: {EMBEDDING_DIM}\")\n\n# Thi·∫øt l·∫≠p thi·∫øt b·ªã\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nmodel_esm2 = model_esm2.to(device)\nmodel_esm2 = model_esm2.eval()\n\nprint(f\"Model loaded. GPU memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\\n\")\n\n# --- H√†m X·ª≠ l√Ω D·ªØ li·ªáu theo Batch ---\ndef process_and_embed_batch(prot_ids, prot_to_seq, prot_to_taxon, model, tokenizer, device, max_length=1024):\n    final_features = []\n\n    for pid in prot_ids:\n        if pid not in prot_to_seq:\n            continue\n        seq = prot_to_seq[pid]\n\n        # --- Chunk sequence n·∫øu qu√° d√†i ---\n        chunks = [seq[i:i+max_length] for i in range(0, len(seq), max_length)]\n        chunk_embeddings = []\n\n        for chunk in chunks:\n            tokens = tokenizer(\n                chunk,\n                return_tensors=\"pt\",\n                padding=True,\n                truncation=True,\n                max_length=max_length,\n                add_special_tokens=False\n            )\n            input_ids = tokens['input_ids'].to(device)\n            attention_mask = tokens['attention_mask'].to(device)\n\n            with torch.no_grad():\n                output = model(input_ids=input_ids, attention_mask=attention_mask)\n                # L·∫•y embedding CLS\n                emb = output.last_hidden_state[:, 0, :]  # [1, hidden_dim]\n                chunk_embeddings.append(emb.squeeze(0))\n\n            del input_ids, attention_mask, output\n            torch.cuda.empty_cache()\n        \n        # --- Trung b√¨nh embedding c√°c chunk ---\n        seq_emb = torch.stack(chunk_embeddings, dim=0).mean(dim=0)  # [hidden_dim]\n\n        # --- Gh√©p Taxon One-Hot ---\n        taxon_vec = prot_taxon_onehot(pid, prot_to_taxon).to(device)\n        features = torch.cat([seq_emb, taxon_vec], dim=0)\n        final_features.append((pid, features))\n\n        del chunk_embeddings, seq_emb, taxon_vec\n        torch.cuda.empty_cache()\n\n    return final_features\n\n\n# --- Ki·ªÉm tra ƒë·ªô d√†i chu·ªói ---\nprint(\"Analyzing sequence lengths...\")\nlengths = [len(prot_to_seq[pid]) for pid in prot_to_seq.keys() if pid in prot_to_seq]\nprint(f\"Total proteins: {len(lengths)}\")\nprint(f\"Min length: {min(lengths)}, Max length: {max(lengths)}\")\nprint(f\"Mean length: {sum(lengths)/len(lengths):.0f}\\n\")\n\n# --- V√≤ng L·∫∑p Ch√≠nh v·ªõi Ti·∫øn ƒê·ªô ---\nfinal_features_list = []\nall_prot_ids = list(prot_to_seq.keys())\nBATCH_SIZE = 128  # X·ª≠ l√Ω t·ª´ng chu·ªói m·ªôt\nMAX_LENGTH = 1024  # Gi·ªõi h·∫°n ƒë·ªô d√†i\n\n# ƒê·∫øm s·ªë protein h·ª£p l·ªá\nvalid_count = sum(1 for pid in all_prot_ids if pid in prot_to_seq and pid in prot_to_taxon)\nprint(f\"Valid proteins (c√≥ c·∫£ sequence v√† taxon): {valid_count}/{len(all_prot_ids)}\\n\")\n\n# T·∫°o progress bar\npbar = tqdm(total=len(all_prot_ids), desc=\"Processing proteins\", unit=\"protein\")\n\nstart_time = time.time()\nerror_count = 0\n\nfor i in range(0, len(all_prot_ids), BATCH_SIZE):\n    batch_ids = all_prot_ids[i:i + BATCH_SIZE]\n    valid_batch_ids = [pid for pid in batch_ids if pid in prot_to_seq and pid in prot_to_taxon]\n    print(i)\n    if valid_batch_ids:\n        try:\n            batch_features = process_and_embed_batch(\n                valid_batch_ids, prot_to_seq, prot_to_taxon, \n                model_esm2, tokenizer, device,\n                max_length=MAX_LENGTH\n            )\n            final_features_list.extend(batch_features)\n        except RuntimeError as e:\n            if \"out of memory\" in str(e):\n                error_count += 1\n                torch.cuda.empty_cache()\n            else:\n                raise e\n\n    pbar.update(len(valid_batch_ids))\n\n\n# --- Th·ªëng k√™ k·∫øt qu·∫£ ---\nelapsed_time = time.time() - start_time\nprint(f\"\\n{'='*60}\")\nprint(f\"‚úì X·ª≠ l√Ω ho√†n t·∫•t!\")\nprint(f\"{'='*60}\")\nprint(f\"T·ªïng proteins x·ª≠ l√Ω: {len(final_features_list)}/{valid_count}\")\nprint(f\"L·ªói OOM: {error_count}\")\nprint(f\"Th·ªùi gian: {elapsed_time/60:.1f} ph√∫t ({elapsed_time:.0f}s)\")\nprint(f\"T·ªëc ƒë·ªô trung b√¨nh: {len(final_features_list)/elapsed_time:.2f} proteins/gi√¢y\")\nprint(f\"K√≠ch th∆∞·ªõc feature: {final_features_list[0][1].shape if final_features_list else 'N/A'}\")\nprint(f\"GPU memory cu·ªëi: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\nprint(f\"{'='*60}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T14:12:35.798013Z","iopub.execute_input":"2025-12-04T14:12:35.798731Z","iopub.status.idle":"2025-12-04T14:12:37.600900Z","shell.execute_reply.started":"2025-12-04T14:12:35.798700Z","shell.execute_reply":"2025-12-04T14:12:37.599714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nprint(\"\\n\" + \"=\"*60)\nprint(\"B·∫Øt ƒë·∫ßu t·ªïng h·ª£p v√† l∆∞u d·ªØ li·ªáu...\")\nprint(\"=\"*60)\n\n# --- 1. T√°ch Protein ID v√† Vector Features ---\nprint(\"\\n[1/4] T√°ch protein IDs v√† features...\")\nprotein_ids = [item[0] for item in final_features_list]\nfeature_tensors = [item[1] for item in final_features_list]\n\nprint(f\"   ‚úì ƒê√£ t√°ch {len(protein_ids)} protein IDs\")\nprint(f\"   ‚úì ƒê√£ t√°ch {len(feature_tensors)} feature vectors\")\n\n# --- 2. Chuy·ªÉn ƒë·ªïi sang NumPy ---\nprint(\"\\n[2/4] Chuy·ªÉn ƒë·ªïi PyTorch tensors sang NumPy arrays...\")\n\n# 2.1. Stack c√°c feature tensors\nprint(\"   - ƒêang stack feature tensors...\")\nstacked_features = torch.stack(feature_tensors)\nprint(f\"   ‚úì K√≠ch th∆∞·ªõc tensor sau khi stack: {stacked_features.shape}\")\n\n# 2.2. Chuy·ªÉn sang CPU v√† NumPy\nprint(\"   - ƒêang chuy·ªÉn sang CPU v√† NumPy...\")\nall_features_np = stacked_features.cpu().numpy()\nprotein_ids_np = np.array(protein_ids)\n\nprint(f\"   ‚úì Features shape: {all_features_np.shape}\")\nprint(f\"   ‚úì Protein IDs shape: {protein_ids_np.shape}\")\nprint(f\"   ‚úì Features dtype: {all_features_np.dtype}\")\n\n# --- 3. T·∫°o th∆∞ m·ª•c output ---\nprint(\"\\n[3/4] Chu·∫©n b·ªã th∆∞ m·ª•c output...\")\noutput_dir = '/kaggle/working/'  # Thay ƒë·ªïi n·∫øu c·∫ßn\nos.makedirs(output_dir, exist_ok=True)\nprint(f\"   ‚úì Th∆∞ m·ª•c output: {output_dir}\")\n\n# --- 4. L∆∞u file .npy ---\nprint(\"\\n[4/4] L∆∞u files .npy...\")\n\n# 4.1. L∆∞u Features (X)\nfeatures_output_path = os.path.join(output_dir, 'cafa56_esm2_taxon_features_X.npy')\nprint(f\"   - ƒêang l∆∞u features...\")\nnp.save(features_output_path, all_features_np)\nfile_size_mb = os.path.getsize(features_output_path) / (1024 * 1024)\nprint(f\"   ‚úÖ ƒê√£ l∆∞u Features t·∫°i: {features_output_path}\")\nprint(f\"      Shape: {all_features_np.shape}\")\nprint(f\"      Size: {file_size_mb:.2f} MB\")\n\n# 4.2. L∆∞u Protein IDs\nids_output_path = os.path.join(output_dir, 'cafa56_protein_ids.npy')\nprint(f\"\\n   - ƒêang l∆∞u protein IDs...\")\nnp.save(ids_output_path, protein_ids_np)\nfile_size_kb = os.path.getsize(ids_output_path) / 1024\nprint(f\"   ‚úÖ ƒê√£ l∆∞u Protein IDs t·∫°i: {ids_output_path}\")\nprint(f\"      Shape: {protein_ids_np.shape}\")\nprint(f\"      Size: {file_size_kb:.2f} KB\")\n\n# --- 5. T√≥m t·∫Øt cu·ªëi c√πng ---\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úì HO√ÄN T·∫§T T·∫§T C·∫¢!\")\nprint(\"=\"*60)\nprint(f\"üìä T·ªïng k·∫øt:\")\nprint(f\"   - S·ªë proteins: {len(protein_ids_np)}\")\nprint(f\"   - Feature dimension: {all_features_np.shape[1]}\")\nprint(f\"   - ESM-2 embedding dim: {EMBEDDING_DIM}\")\nprint(f\"   - Taxon one-hot dim: {num_taxon}\")\nprint(f\"   - Total feature dim: {EMBEDDING_DIM + num_taxon}\")\nprint(f\"\\nüìÅ Files ƒë√£ l∆∞u:\")\nprint(f\"   1. {features_output_path}\")\nprint(f\"   2. {ids_output_path}\")\nprint(\"=\"*60 + \"\\n\")\n\n# --- 6. Ki·ªÉm tra t√≠nh to√†n v·∫πn d·ªØ li·ªáu (Optional) ---\nprint(\"üîç Ki·ªÉm tra t√≠nh to√†n v·∫πn d·ªØ li·ªáu...\")\ntry:\n    # Load l·∫°i ƒë·ªÉ ki·ªÉm tra\n    loaded_features = np.load(features_output_path)\n    loaded_ids = np.load(ids_output_path)\n\n    \n    print(\"   ‚úÖ T·∫•t c·∫£ files ƒë√£ ƒë∆∞·ª£c l∆∞u ƒë√∫ng v√† c√≥ th·ªÉ load l·∫°i!\")\n    print(f\"   ‚úÖ Verified {loaded_features.shape[0]} proteins\")\n    \nexcept Exception as e:\n    print(f\"   ‚ö†Ô∏è L·ªói khi ki·ªÉm tra: {e}\")\n\nprint(\"\\nüéâ Done! B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng c√°c file .npy n√†y cho training.\\n\")\n\n# --- 7. H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng ---\nprint(\"üí° C√°ch load d·ªØ li·ªáu sau n√†y:\")\nprint(\"-\" * 60)\nprint(\"import numpy as np\")\nprint(f\"X = np.load('{features_output_path}')\")\nprint(f\"protein_ids = np.load('{ids_output_path}')\")\nprint(\"print(f'X shape: {X.shape}')\")\nprint(\"print(f'IDs shape: {protein_ids.shape}')\")\nprint(\"-\" * 60 + \"\\n\")\n\nimport numpy as np\nX = np.load('/kaggle/working/cafa56_esm2_taxon_features_X.npy')\nprotein_ids = np.load('/kaggle/working/cafa56_protein_ids.npy')\nprint(f'X shape: {X.shape}')\nprint(f'IDs shape: {protein_ids.shape}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T07:52:45.290021Z","iopub.execute_input":"2025-12-03T07:52:45.290542Z","iopub.status.idle":"2025-12-03T07:52:45.853400Z","shell.execute_reply.started":"2025-12-03T07:52:45.290517Z","shell.execute_reply":"2025-12-03T07:52:45.852549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n\n# protein_ids = np.load('/kaggle/input/cafa-m56/CAFA56/cafa56_protein_ids.npy')\n# X = np.load('/kaggle/input/cafa-m56/CAFA56/cafa56_esm2_taxon_features_X.npy')\n\n# print(f'X shape: {X.shape}')\n# print(f'IDs shape: {protein_ids.shape}')\n# print(f'---')\n# # In th√™m th√¥ng tin chi ti·∫øt\n# print(f'X dtype: {X.dtype}')\n# print(f'X ndim: {X.ndim}')\n# print(f'IDs dtype: {protein_ids.dtype}')\n# print(f'IDs ndim: {protein_ids.ndim}')\n# # In ra m·ªôt v√†i ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n ƒë·ªÉ ki·ªÉm tra n·ªôi dung\n# print(f'---')\n# print(f'X first 5 rows:\\n{X[:5]}') \n# print(f'IDs first 5 elements: {protein_ids[:5]}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:03:04.285807Z","iopub.execute_input":"2025-12-07T02:03:04.286389Z","iopub.status.idle":"2025-12-07T02:03:04.471118Z","shell.execute_reply.started":"2025-12-07T02:03:04.286366Z","shell.execute_reply":"2025-12-07T02:03:04.470283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n\n# # ================================\n# # 1. Load X v√† IDs ƒë√£ n√©n s·∫µn\n# # ================================\n\n# IDs = np.load('/kaggle/input/cafa-m56/CAFA56/cafa56_protein_ids.npy')\n# X = np.load('/kaggle/input/cafa-m56/CAFA56/cafa56_esm2_taxon_features_X.npy')\n\n# print(\"Loaded X:\", X.shape)\n# print(\"Loaded IDs:\", IDs.shape)\n\n# # Map ID ‚Üí index trong X\n# id_to_index = {pid: i for i, pid in enumerate(IDs)}\n\n# # ================================\n# # 2. Load b·∫£ng GO annotation train\n# # ================================\n# # Format file: EntryID \\t term \\t aspect\n# df = pd.read_csv(\"/kaggle/input/cafa-m56/CAFA56/CAFA56_train_terms.tsv\", sep=\"\\t\")\n\n# print(\"Annotation rows:\", len(df))\n# df.head()\n\n# # ================================\n# # 3. L·∫•y danh s√°ch t·∫•t c·∫£ GO terms trong train\n# # ================================\n# all_go_terms = sorted(df[\"term\"].unique())\n# num_go = len(all_go_terms)\n\n# print(\"Number of GO terms =\", num_go)\n\n# # Map GO ‚Üí c·ªôt c·ªßa Y\n# go_to_idx = {go: i for i, go in enumerate(all_go_terms)}\n\n# # ================================\n# # 4. T·∫°o ma tr·∫≠n Y (multi-hot)\n# # ================================\n# N = len(IDs)\n# Y = np.zeros((N, num_go), dtype=np.float32)\n\n# missing = 0\n\n# for _, row in df.iterrows():\n#     pid = row[\"EntryID\"]\n#     go  = row[\"term\"]\n\n#     if pid not in id_to_index:\n#         missing += 1\n#         continue\n\n#     i = id_to_index[pid]  # row trong X\n#     j = go_to_idx[go]     # c·ªôt GO\n#     Y[i, j] = 1.0\n\n# print(\"Proteins in annotation but NOT in X:\", missing)\n\n# # ================================\n# # 5. L∆∞u Y v√† danh s√°ch GO terms\n# # ================================\n# np.save(\"/kaggle/working/Y.npy\", Y)\n# np.save(\"/kaggle/working/GO_terms.npy\", np.array(all_go_terms))\n\n# print(\"\\n===== DONE =====\")\n# print(\"X shape:\", X.shape)\n# print(\"Y shape:\", Y.shape)\n# print(\"GO_terms shape:\", len(all_go_terms))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:13:23.917990Z","iopub.execute_input":"2025-12-07T02:13:23.918142Z","iopub.status.idle":"2025-12-07T02:17:51.756100Z","shell.execute_reply.started":"2025-12-07T02:13:23.918127Z","shell.execute_reply":"2025-12-07T02:17:51.755078Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy import sparse\nfrom collections import Counter\n\n# =============== OBO PARSER ===============\ndef parse_obo(obo_path):\n    parents = {}\n    children = {}\n    current = None\n\n    with open(obo_path, \"r\") as f:\n        for line in f:\n            line = line.strip()\n\n            if line == \"[Term]\":\n                current = None\n                continue\n\n            if line.startswith(\"id: GO:\"):\n                current = line.split(\"id: \")[1]\n                parents[current] = []\n                children[current] = []\n                continue\n\n            if line.startswith(\"is_a:\") and current:\n                parent = line.split(\"is_a: \")[1].split(\" ! \")[0]\n                parents[current].append(parent)\n\n    for child, plist in parents.items():\n        for p in plist:\n            if p not in children:\n                children[p] = []\n            children[p].append(child)\n\n    return parents, children\n\n\ndef filter_leaf_terms(filtered_terms, children):\n    terms_set = set(filtered_terms)\n    leaf_terms = []\n\n    for go in filtered_terms:\n        child_list = children.get(go, [])\n        if not any(c in terms_set for c in child_list):\n            leaf_terms.append(go)\n\n    return leaf_terms\n\n# ==========================================\n\nobo = ('/kaggle/input/cafa56-end/go-basic.obo')\nIDs = np.load('/kaggle/input/cafa56-end/650_protein_ids_INPUT.npy')\nX = np.load('/kaggle/input/cafa56-end/650_taxon_features_X_INPUT.npy')\n\nid_to_index = {pid: i for i, pid in enumerate(IDs)}\n\nparents, children = parse_obo(obo)\ndf = pd.read_csv(\"/kaggle/input/cafa-m56/CAFA56/CAFA56_train_terms.tsv\", sep=\"\\t\")\n\nMIN_FREQ = 21\naspects = [\"C\", \"P\", \"F\"]\n\nfor asp in aspects:\n    df_asp = df[df[\"aspect\"] == asp]\n    term_counts = Counter(df_asp[\"term\"])\n\n    filtered_terms = sorted([go for go, c in term_counts.items() if c >= MIN_FREQ])\n    print(f\"[{asp}] freq‚â•{MIN_FREQ}:\", len(filtered_terms))\n\n    # NEW: apply leaf filter\n    # filtered_terms = filter_leaf_terms(filtered_terms, children)\n    # print(f\"[{asp}] leaf-only terms:\", len(filtered_terms))\n\n    go_to_idx = {go: i for i, go in enumerate(filtered_terms)}\n    num_go = len(filtered_terms)\n\n    rows, cols = [], []\n    missing = 0\n    N = len(IDs)\n\n    for _, row in df_asp.iterrows():\n        pid = row[\"EntryID\"]\n        go = row[\"term\"]\n\n        if go not in go_to_idx:\n            continue\n        if pid not in id_to_index:\n            missing += 1\n            continue\n\n        rows.append(id_to_index[pid])\n        cols.append(go_to_idx[go])\n\n    data = np.ones(len(rows), dtype=np.float32)\n    Y_sparse = sparse.coo_matrix((data, (rows, cols)), shape=(N, num_go))\n\n    sparse.save_npz(f\"/kaggle/working/Y_{asp}.npz\", Y_sparse)\n    np.save(f\"/kaggle/working/GO_terms_{asp}.npy\", np.array(filtered_terms))\n\n    print(f\"{asp} done: {num_go} GO terms, missing proteins: {missing}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T07:30:02.454185Z","iopub.execute_input":"2025-12-11T07:30:02.454506Z","iopub.status.idle":"2025-12-11T07:34:12.173711Z","shell.execute_reply.started":"2025-12-11T07:30:02.454482Z","shell.execute_reply":"2025-12-11T07:34:12.172741Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy import sparse\nfrom collections import Counter\n\n# ================================\n# 1. Load X v√† IDs\n# ================================\nIDs = np.load('/kaggle/input/cafa56-end/650_protein_ids_INPUT.npy')\nX = np.load('/kaggle/input/cafa56-end/650_taxon_features_X_INPUT.npy')\n\nprint(\"Loaded X:\", X.shape)\nprint(\"Loaded IDs:\", IDs.shape)\n\nid_to_index = {pid: i for i, pid in enumerate(IDs)}\n\n# ================================\n# 2. Load b·∫£ng GO annotation train\n# ================================\ndf = pd.read_csv(\"/kaggle/input/cafa-m56/CAFA56/CAFA56_train_terms.tsv\", sep=\"\\t\")\nprint(\"Annotation rows:\", len(df))\n\n# ================================\n# 3. Filter GO terms by frequency ‚â• MIN_FREQ\n# ================================\nMIN_FREQ = 21\n\n# ---- Count GO terms ----\nterm_counts = Counter(df[\"term\"])\n\n# ---- Keep all terms with freq ‚â• MIN_FREQ ----\nfiltered_terms = sorted([\n    go for go, c in term_counts.items() if c >= MIN_FREQ\n])\n\nprint(f\"Original GO terms: {len(term_counts)}\")\nprint(f\"Filtered GO terms (freq ‚â• {MIN_FREQ}): {len(filtered_terms)}\")\n\n# Mapping GO ‚Üí index\ngo_to_idx = {go: i for i, go in enumerate(filtered_terms)}\nnum_go = len(filtered_terms)\n\nN = len(IDs)\nrows = []\ncols = []\nmissing = 0\n\n# ================================\n# 4. Build sparse Y (NO OBO FILTER)\n# ================================\nfor _, row in df.iterrows():\n    pid = row[\"EntryID\"]\n    go = row[\"term\"]\n\n    # Skip infrequent GO\n    if go not in go_to_idx:\n        continue\n\n    # Skip proteins not in X\n    if pid not in id_to_index:\n        missing += 1\n        continue\n\n    i = id_to_index[pid]\n    j = go_to_idx[go]\n    rows.append(i)\n    cols.append(j)\n\ndata = np.ones(len(rows), dtype=np.float32)\nY_sparse = sparse.coo_matrix((data, (rows, cols)), shape=(N, num_go))\n\n# ================================\n# 5. Save outputs\n# ================================\nsparse.save_npz(\"/kaggle/working/Y.npz\", Y_sparse)\nnp.save(\"/kaggle/working/GO_terms.npy\", np.array(filtered_terms))\n\nprint(\"\\n==== DONE ====\")\nprint(f\"Stored {num_go} GO terms in Y.npz\")\nprint(\"Proteins missing in X:\", missing)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T11:11:51.399861Z","iopub.execute_input":"2025-12-11T11:11:51.400054Z","iopub.status.idle":"2025-12-11T11:16:00.818954Z","shell.execute_reply.started":"2025-12-11T11:11:51.400035Z","shell.execute_reply":"2025-12-11T11:16:00.817931Z"}},"outputs":[{"name":"stdout","text":"Loaded X: (144096, 1291)\nLoaded IDs: (144096,)\nAnnotation rows: 5410821\nOriginal GO terms: 32347\nFiltered GO terms (freq ‚â• 0): 32347\n\n==== DONE ====\nStored 32347 GO terms in Y.npz\nProteins missing in X: 25447\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# import numpy as np\n# from scipy import sparse\n\n# # Load Y dense\n# Y = np.load(\"/kaggle/working/Y.npy\", allow_pickle=False)\n# GO_terms = np.load(\"/kaggle/working/GO_terms.npy\", allow_pickle=True)\n\n# print(\"Dense Y shape:\", Y.shape)\n\n# # Chuy·ªÉn sang sparse\n# Y_sparse = sparse.csr_matrix(Y)\n# sparse.save_npz(\"/kaggle/working/Y_sparse.npz\", Y_sparse)\n# print(\"Saved sparse Y at /kaggle/working/Y_sparse.npz\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:18:20.595687Z","iopub.execute_input":"2025-12-07T02:18:20.596432Z","iopub.status.idle":"2025-12-07T02:20:17.244403Z","shell.execute_reply.started":"2025-12-07T02:18:20.596407Z","shell.execute_reply":"2025-12-07T02:20:17.243606Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# from scipy import sparse\n\n# # Load Y sparse\n# Y_sparse = sparse.load_npz(\"/kaggle/working/Y_sparse.npz\")\n# GO_terms = np.load(\"/kaggle/working/GO_terms.npy\", allow_pickle=True)\n\n# print(\"Sparse Y loaded\")\n# print(\"Type:\", type(Y_sparse))\n# print(\"Shape:\", Y_sparse.shape)\n# print(\"Number of non-zero entries:\", Y_sparse.nnz)\n# print(\"Density (non-zero fraction):\", Y_sparse.nnz / (Y_sparse.shape[0] * Y_sparse.shape[1]))\n\n# # Ki·ªÉm tra v√†i d√≤ng ƒë·∫ßu\n# num_rows_to_check = 5\n# for i in range(num_rows_to_check):\n#     row = Y_sparse.getrow(i).toarray()  # Chuy·ªÉn sang dense t·∫°m th·ªùi ƒë·ªÉ ki·ªÉm tra\n#     print(f\"\\nRow {i} non-zero indices:\", np.nonzero(row)[1])\n#     print(f\"Row {i} values:\", row[0, np.nonzero(row)[1]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:20:38.184426Z","iopub.execute_input":"2025-12-07T02:20:38.185021Z","iopub.status.idle":"2025-12-07T02:20:38.353217Z","shell.execute_reply.started":"2025-12-07T02:20:38.184998Z","shell.execute_reply":"2025-12-07T02:20:38.352603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from scipy.sparse import load_npz\n\n# # --- Load d·ªØ li·ªáu ---\n# X = np.load('/kaggle/input/cafa-m56/CAFA56/cafa56_esm2_taxon_features_X.npy')\n# protein_ids = np.load('/kaggle/input/cafa-m56/CAFA56/cafa56_protein_ids.npy')\n# Y_sparse = load_npz(\"/kaggle/working/Y_sparse.npz\")  # CSR sparse\n\n# # File train GO\n# train_go_file = \"/kaggle/input/cafa-m56/CAFA56/CAFA56_train_terms.tsv\"  # s·ª≠a ƒë∆∞·ªùng d·∫´n n·∫øu c·∫ßn\n# train_df = pd.read_csv(train_go_file, sep='\\t')\n\n# # --- T·∫°o dict protein_id -> list GO indices ---\n# # Tr∆∞·ªõc h·∫øt, c·∫ßn danh s√°ch t·∫•t c·∫£ GO terms\n# all_go_terms = np.load(\"/kaggle/working/GO_terms.npy\", allow_pickle=True)\n# go_to_index = {go:i for i, go in enumerate(all_go_terms)}\n\n# protein_to_go_indices = {}\n# for pid, go, aspect in zip(train_df['EntryID'], train_df['term'], train_df['aspect']):\n#     if pid not in protein_to_go_indices:\n#         protein_to_go_indices[pid] = []\n#     protein_to_go_indices[pid].append(go_to_index[go])\n\n# # --- H√†m ki·ªÉm tra ng·∫´u nhi√™n n protein ---\n# def check_features_labels_match(n=5, seed=42):\n#     np.random.seed(seed)\n#     indices = np.random.choice(len(protein_ids), n, replace=False)\n    \n#     for idx in indices:\n#         pid = protein_ids[idx]\n#         row_sparse = Y_sparse[idx]\n#         row_indices = row_sparse.nonzero()[1]  # ch·ªâ s·ªë c·ªôt c√≥ nh√£n\n#         train_indices = protein_to_go_indices.get(pid, [])\n        \n#         print(f\"Protein ID: {pid}\")\n#         print(f\"Indices in Y_sparse: {sorted(row_indices)}\")\n#         print(f\"Indices in train file: {sorted(train_indices)}\")\n#         print(f\"Match? {set(row_indices) == set(train_indices)}\")\n#         print(\"-\"*50)\n\n# # --- Ch·∫°y ki·ªÉm tra ---\n# check_features_labels_match(n=10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:20:54.325172Z","iopub.execute_input":"2025-12-07T02:20:54.325441Z","iopub.status.idle":"2025-12-07T02:21:01.700173Z","shell.execute_reply.started":"2025-12-07T02:20:54.325422Z","shell.execute_reply":"2025-12-07T02:21:01.699531Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# from torch.utils.data import Dataset, DataLoader\n# import numpy as np\n# from scipy.sparse import csr_matrix, load_npz\n\n# # --- Load d·ªØ li·ªáu ---\n# X = np.load('/kaggle/input/cafa-m56/CAFA56/cafa56_esm2_taxon_features_X.npy')\n# protein_ids = np.load('/kaggle/input/cafa-m56/CAFA56/cafa56_protein_ids.npy')\n# X_test = np.load('/kaggle/input/cafa-m56/CAFA56/cafa56_X_test.npy')\n# X_test_protein_ids = np.load('/kaggle/input/cafa-m56/CAFA56/cafa56_protein_ids_test.npy')\n# Y_sparse = load_npz(\"/kaggle/input/cafa56-y-label/cafa56_Y_sparse.npz\")  # CSR sparse\n# all_go_terms = np.load(\"/kaggle/input/cafa56-y-label/cafa56_GO_terms.npy\", allow_pickle=True)\n\n# # --- Dataset class ---\n# class ProteinDataset(Dataset):\n#     def __init__(self, X, Y_sparse):\n#         self.X = torch.tensor(X, dtype=torch.float32)\n#         # Convert sparse Y to dense tensor khi training\n#         self.Y = torch.tensor(Y_sparse.toarray(), dtype=torch.float32)\n\n#     def __len__(self):\n#         return self.X.shape[0]\n\n#     def __getitem__(self, idx):\n#         return self.X[idx], self.Y[idx]\n\n# # --- T·∫°o dataset ---\n# dataset = ProteinDataset(X, Y_sparse)\n\n# # --- Chia train/test ---\n# from sklearn.model_selection import train_test_split\n\n# indices = np.arange(len(dataset))\n# train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n\n# from torch.utils.data import Subset\n\n# train_dataset = Subset(dataset, train_idx)\n# test_dataset = Subset(dataset, test_idx)\n\n# # --- DataLoader ---\n# batch_size = 64\n# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# # --- Ki·ªÉm tra ---\n# for X_batch, Y_batch in train_loader:\n#     print(X_batch.shape)  # [batch_size, feature_dim]\n#     print(Y_batch.shape)  # [batch_size, num_labels]\n#     break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:26:36.103703Z","iopub.execute_input":"2025-12-07T02:26:36.104461Z","iopub.status.idle":"2025-12-07T02:26:55.048396Z","shell.execute_reply.started":"2025-12-07T02:26:36.104430Z","shell.execute_reply":"2025-12-07T02:26:55.047655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom scipy.sparse import load_npz, csr_matrix\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom sklearn.model_selection import train_test_split\n\ndef load_data():\n    X = np.load('/kaggle/input/cafa56-end/650_taxon_features_X_INPUT.npy')\n    protein_ids = np.load('/kaggle/input/cafa56-end/650_protein_ids_INPUT.npy')\n    X_test = np.load('/kaggle/input/cafa56-end/X_test.npy')\n    X_test_protein_ids = np.load('/kaggle/input/cafa56-end/protein_ids_test.npy')\n    Y_sparse = load_npz(\"/kaggle/working/Y.npz\")  # CSR sparse\n    GO = np.load(\"/kaggle/working/GO_terms.npy\", allow_pickle=True)\n    return X, protein_ids, X_test, X_test_protein_ids, Y_sparse, GO\n\nclass ProteinDataset(Dataset):\n    def __init__(self, X, Y_sparse):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.Y = torch.tensor(Y_sparse.toarray(), dtype=torch.float32)\n\n    def __len__(self):\n        return self.X.shape[0]\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.Y[idx]\n\ndef check_integrity(X_train, ids_train, X_test, ids_test, Y_sparse, GO_terms):\n    print(\"== BASIC SHAPE CHECK ==\")\n    n_X = X_train.shape[0]\n    n_ids = len(ids_train)\n    n_Y = Y_sparse.shape[0]\n    n_labels = Y_sparse.shape[1]\n    n_GO = len(GO_terms)\n\n    print(f\"X_train : {X_train.shape}\")\n    print(f\"ids_train count : {n_ids}\")\n    print(f\"Y_sparse shape : {Y_sparse.shape}\")\n    print(f\"GO terms count : {n_GO}\")\n    print(f\"X_test  : {X_test.shape}\")\n    print(f\"ids_test count : {len(ids_test)}\")\n\n    if n_X != n_ids:\n        print(\"‚úó FAIL: S·ªë l∆∞·ª£ng m·∫´u X_train v√† ids_train kh√¥ng kh·ªõp!\")\n    else:\n        print(\"‚úì ids_train kh·ªõp s·ªë m·∫´u X_train\")\n\n    if n_X != n_Y:\n        print(\"‚úó FAIL: S·ªë l∆∞·ª£ng X_train v√† Y labels kh√¥ng kh·ªõp!\")\n    else:\n        print(\"‚úì S·ªë m·∫´u X_train & Y labels kh·ªõp nhau\")\n\n    if n_labels != n_GO:\n        print(\"‚úó FAIL: S·ªë chi·ªÅu Y (labels) v√† s·ªë GO terms kh√¥ng kh·ªõp!\")\n    else:\n        print(\"‚úì S·ªë labels kh·ªõp s·ªë GO terms\")\n\n    # Check X_test feature dim matches X_train\n    if X_test.shape[1] != X_train.shape[1]:\n        print(\"‚úó FAIL: s·ªë chi·ªÅu feature X_test v√† X_train KH√îNG kh·ªõp!\")\n    else:\n        print(\"‚úì Feature dimension X_test & X_train kh·ªõp\")\n\n    print(\"\\n== SAMPLE IDS EXAMPLE ==\")\n    for i in [0, 1, min(5, n_ids - 1)]:\n        print(f\"Train sample {i} : id = {ids_train[i]}\")\n\n    for i in [0, 1, min(5, len(ids_test) - 1)]:\n        print(f\"Test sample {i}  : id = {ids_test[i]}\")\n\n    # Optionally: check some Y vectors (sparsity / zero-rows)\n    Y_dense = Y_sparse.toarray()\n    n_zero_rows = np.sum((Y_dense.sum(axis=1) == 0))\n    print(f\"\\nTrong Y labels c√≥ {n_zero_rows}/{n_Y} m·∫´u to√†n zero-label (kh√¥ng GO term n√†o).\")\n\ndef build_and_run_loader(X_train, Y_sparse, batch_size=64, random_state=42):\n    dataset = ProteinDataset(X_train, Y_sparse)\n    indices = np.arange(len(dataset))\n    train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=random_state)\n    train_ds = Subset(dataset, train_idx)\n    test_ds = Subset(dataset, test_idx)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n\n    print(\"\\n== DATALOADER BATCH SHAPE CHECK ==\")\n    for Xb, Yb in train_loader:\n        print(\"Train batch Xb.shape:\", Xb.shape)\n        print(\"Train batch Yb.shape:\", Yb.shape)\n        break\n    for Xb, Yb in test_loader:\n        print(\"Test  batch Xb.shape:\", Xb.shape)\n        print(\"Test  batch Yb.shape:\", Yb.shape)\n        break\n\n    return train_loader, test_loader\n\nif __name__ == \"__main__\":\n    X, ids, X_test, ids_test, Y_sparse, GO = load_data()\n    check_integrity(X, ids, X_test, ids_test, Y_sparse, GO)\n    build_and_run_loader(X, Y_sparse)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T11:22:00.792908Z","iopub.execute_input":"2025-12-11T11:22:00.793858Z","iopub.status.idle":"2025-12-11T11:22:53.615587Z","shell.execute_reply.started":"2025-12-11T11:22:00.793826Z","shell.execute_reply":"2025-12-11T11:22:53.614647Z"}},"outputs":[{"name":"stdout","text":"== BASIC SHAPE CHECK ==\nX_train : (144096, 1291)\nids_train count : 144096\nY_sparse shape : (144096, 32347)\nGO terms count : 32347\nX_test  : (224309, 1291)\nids_test count : 224309\n‚úì ids_train kh·ªõp s·ªë m·∫´u X_train\n‚úì S·ªë m·∫´u X_train & Y labels kh·ªõp nhau\n‚úì S·ªë labels kh·ªõp s·ªë GO terms\n‚úì Feature dimension X_test & X_train kh·ªõp\n\n== SAMPLE IDS EXAMPLE ==\nTrain sample 0 : id = P20536\nTrain sample 1 : id = O73864\nTrain sample 5 : id = P33681\nTest sample 0  : id = A0A0C5B5G6\nTest sample 1  : id = A0A1B0GTW7\nTest sample 5  : id = A1A4S6\n\nTrong Y labels c√≥ 0/144096 m·∫´u to√†n zero-label (kh√¥ng GO term n√†o).\n\n== DATALOADER BATCH SHAPE CHECK ==\nTrain batch Xb.shape: torch.Size([64, 1291])\nTrain batch Yb.shape: torch.Size([64, 32347])\nTest  batch Xb.shape: torch.Size([64, 1291])\nTest  batch Yb.shape: torch.Size([64, 32347])\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}