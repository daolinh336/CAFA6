{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13958805,"sourceType":"datasetVersion","datasetId":8897799}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n!pip install biopython obonet --quiet\n!pip install transformers biopython --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-03T12:20:45.489400Z","iopub.execute_input":"2025-12-03T12:20:45.489595Z","iopub.status.idle":"2025-12-03T12:20:54.313333Z","shell.execute_reply.started":"2025-12-03T12:20:45.489578Z","shell.execute_reply":"2025-12-03T12:20:54.312280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import EsmTokenizer, EsmModel\nfrom Bio import SeqIO\nimport os\nfrom tqdm.auto import tqdm\nimport gc\nimport numpy as np\n\n# --- Danh sách Top Taxa ---\ntop_taxa = [\"9606\", \"10090\", \"3702\", \"559292\", \"10116\", \"284812\", \n            \"83333\", \"7227\", \"6239\", \"83332\"]\ntaxon_to_index_top = {taxon: i for i, taxon in enumerate(top_taxa)}\nothers_index = len(top_taxa)\nnum_taxon_top = len(top_taxa) + 1  # 11 chiều\n\n# --- Hàm tạo vector taxonomy one-hot ---\ndef prot_taxon_onehot(taxon_id):\n    vec = torch.zeros(num_taxon_top, dtype=torch.float32)\n    if taxon_id in taxon_to_index_top:\n        vec[taxon_to_index_top[taxon_id]] = 1\n    else:\n        vec[others_index] = 1\n    return vec\n\n# --- Load ESM-2 ---\nmodel_name = \"facebook/esm2_t30_150M_UR50D\"\ntokenizer = EsmTokenizer.from_pretrained(model_name)\nmodel_esm2 = EsmModel.from_pretrained(model_name)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_esm2 = model_esm2.to(device)\nmodel_esm2.eval()\nEMBEDDING_DIM = model_esm2.config.hidden_size\nprint(f\"ESM embedding dim: {EMBEDDING_DIM}, Device: {device}\")\n\n# --- Parse FASTA ---\nfasta_file = \"/kaggle/input/train-test-cafa6-v2/Test/testsuperset.fasta\"\nprot_to_seq = {}\nprot_to_taxon = {}\n\nfor record in SeqIO.parse(fasta_file, \"fasta\"):\n    # Header dạng: >ProteinID TaxonID\n    parts = record.id.split()\n    pid = parts[0]\n    taxon_id = parts[1] if len(parts) > 1 else None\n    prot_to_seq[pid] = str(record.seq)\n    prot_to_taxon[pid] = taxon_id\n\nprint(f\"Total proteins in test set: {len(prot_to_seq)}\")\n\n# --- Hàm process batch ---\ndef process_and_embed_batch(prot_ids, max_length=1024):\n    features_list = []\n    for pid in prot_ids:\n        seq = prot_to_seq[pid]\n        taxon_id = prot_to_taxon.get(pid, None)\n        \n        # Chunk nếu quá dài\n        chunks = [seq[i:i+max_length] for i in range(0, len(seq), max_length)]\n        chunk_embeddings = []\n\n        for chunk in chunks:\n            tokens = tokenizer(\n                chunk,\n                return_tensors=\"pt\",\n                padding=True,\n                truncation=True,\n                max_length=max_length,\n                add_special_tokens=False\n            )\n            input_ids = tokens['input_ids'].to(device)\n            attention_mask = tokens['attention_mask'].to(device)\n\n            with torch.no_grad():\n                output = model_esm2(input_ids=input_ids, attention_mask=attention_mask)\n                emb = output.last_hidden_state[:, 0, :]  # CLS token\n                chunk_embeddings.append(emb.squeeze(0))\n\n            del input_ids, attention_mask, output\n            torch.cuda.empty_cache()\n\n        seq_emb = torch.stack(chunk_embeddings, dim=0).mean(dim=0)\n        taxon_vec = prot_taxon_onehot(taxon_id).to(device)\n        features = torch.cat([seq_emb, taxon_vec], dim=0)\n        features_list.append((pid, features.cpu()))\n        \n        del chunk_embeddings, seq_emb, taxon_vec\n        torch.cuda.empty_cache()\n\n    return features_list\n\n# --- Process tất cả protein với progress bar ---\nall_prot_ids = list(prot_to_seq.keys())\nfinal_features_list = []\n\npbar = tqdm(total=len(all_prot_ids), desc=\"Processing proteins\", unit=\"protein\")\nx = 0\nfor pid in all_prot_ids:\n    if x % 2000 == 0:\n        print(x)\n    batch_feat = process_and_embed_batch([pid])\n    final_features_list.extend(batch_feat)\n    pbar.update(1)\n\n# --- Lưu ra file npy ---\nprotein_ids = [item[0] for item in final_features_list]\nfeature_tensors = [item[1] for item in final_features_list]\nX_test = torch.stack(feature_tensors).numpy()\nprotein_ids_test = np.array(protein_ids)\n\noutput_dir = \"/kaggle/working/\"\nos.makedirs(output_dir, exist_ok=True)\n\nnp.save(os.path.join(output_dir, \"X_test.npy\"), X_test)\nnp.save(os.path.join(output_dir, \"protein_ids_test.npy\"), protein_ids_test)\n\nprint(f\"✓ X_test shape: {X_test.shape}\")\nprint(f\"✓ protein_ids_test shape: {protein_ids_test.shape}\")\nprint(\"Saved X_test.npy and protein_ids_test.npy\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T12:20:54.315071Z","iopub.execute_input":"2025-12-03T12:20:54.315357Z","iopub.status.idle":"2025-12-03T16:01:27.197465Z","shell.execute_reply.started":"2025-12-03T12:20:54.315332Z","shell.execute_reply":"2025-12-03T16:01:27.196612Z"}},"outputs":[],"execution_count":null}]}