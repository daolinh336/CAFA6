{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14021088,"sourceType":"datasetVersion","datasetId":8930262},{"sourceId":14094271,"sourceType":"datasetVersion","datasetId":8975090},{"sourceId":14108008,"sourceType":"datasetVersion","datasetId":8976080}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# deepgo_pipeline_with_val_loss.py\nimport os\nimport time\nimport csv\nimport numpy as np\nfrom scipy.sparse import load_npz\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# ---------------------------\n# CONFIG\n# ---------------------------\nSEED = 42\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 16       # giảm nếu OOM\nEPOCHS = 40\nLR = 5e-4\nWEIGHT_DECAY = 1e-4\nCLIP_NORM = 5.0\nFEATURE_DROPOUT = 0.2\nLABEL_SMOOTH = 0.03\nTOP_K = 150\nALPHA = 0.2\nTHRESH_GRID = np.arange(0.0, 0.201, 0.005)\n\nOUT_MODEL = \"/kaggle/working/best_model.pt\"\nOUT_SUBMIT = \"/kaggle/working/submission.tsv\"\n\nOUT_MODEL2 = \"/kaggle/working/best_model2.pt\"\nOUT_SUBMIT2 = \"/kaggle/working/submission2.tsv\"\n\n# reproducibility\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# ---------------------------\n# LOAD DATA\n# ---------------------------\n\n\n\nX_train = np.load('/kaggle/input/cafa56-end/650_taxon_features_X_INPUT.npy')\nX_test = np.load('/kaggle/input/cafa56-end/X_test.npy')\nids_test = np.load('/kaggle/input/cafa56-end/protein_ids_test.npy')\nY_sparse = load_npz(\"/kaggle/input/cafa56-end/Y_full.npz\")  # CSR sparse\nGO_terms = np.load(\"/kaggle/input/cafa56-end/GO_terms_full.npy\", allow_pickle=True)\n\n# normalization\nglobal_mean = X_train.mean(axis=0).astype(np.float32)\nglobal_std  = X_train.std(axis=0).astype(np.float32) + 1e-6\n\n# train/val split\nY_sparse = Y_sparse.tocsr()\nrow_nnz = np.diff(Y_sparse.indptr)\nvalid_idx = np.where(row_nnz > 0)[0]\ntrain_idx, val_idx = train_test_split(valid_idx, test_size=0.1, random_state=SEED, shuffle=True)\n\n# pos_weight\ntrain_sparse = Y_sparse[train_idx]\nlabel_freq = np.array(train_sparse.sum(axis=0)).squeeze()\nN_train = len(train_idx)\npos_weight_arr = (N_train - label_freq) / (label_freq + 1e-8)\npos_weight_arr = np.clip(pos_weight_arr, 1.0, 5.0)\npos_weight = torch.tensor(pos_weight_arr, dtype=torch.float32).to(DEVICE)\n\n# IA vector\nIA_dict = {}\nwith open(\"/kaggle/input/cafa56-end/IA.tsv\") as f:\n    for line in f:\n        go, value = line.strip().split(\"\\t\")\n        IA_dict[go] = float(value)\nIA_vec = np.array([IA_dict.get(go, 0.0) for go in GO_terms], dtype=np.float32)\n\n# ontology mapping\ngo2asp = {}\nwith open(\"/kaggle/input/mapping-wf1/go_to_aspect.tsv\") as f:\n    next(f)\n    for line in f:\n        go, asp = line.strip().split(\",\")\n        go2asp[go] = asp\nidx_MF = [i for i, go in enumerate(GO_terms) if go2asp.get(go) == \"F\"]\nidx_CC = [i for i, go in enumerate(GO_terms) if go2asp.get(go) == \"C\"]\nidx_BP = [i for i, go in enumerate(GO_terms) if go2asp.get(go) == \"P\"]\n\nIA_vec_torch = torch.tensor(IA_vec, dtype=torch.float32).to(DEVICE)\n\n# ---------------------------\n# DATASET\n# ---------------------------\nclass ProteinDataset(Dataset):\n    def __init__(self, X, Y_sparse=None, indices=None, mean=None, std=None, feature_dropout=0.0, train=True):\n        self.X = X\n        self.Y = Y_sparse\n        self.indices = np.array(indices) if indices is not None else np.arange(X.shape[0])\n        self.mean = mean\n        self.std = std\n        self.feature_dropout = feature_dropout\n        self.train = train\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, idx):\n        i = int(self.indices[idx])\n        x = self.X[i].astype(np.float32)\n        if self.mean is not None and self.std is not None:\n            x = (x - self.mean) / self.std\n        if self.train and self.feature_dropout > 0.0 and np.random.rand() < 0.5:\n            mask = (np.random.rand(x.shape[0]) >= self.feature_dropout).astype(np.float32)\n            x = x * mask\n        x = torch.from_numpy(x)\n        if self.Y is not None:\n            y = torch.from_numpy(self.Y[i].toarray().squeeze().astype(np.float32))\n            return x, y\n        else:\n            return x\n\ntrain_ds = ProteinDataset(X_train, Y_sparse, train_idx, global_mean, global_std, FEATURE_DROPOUT, True)\nval_ds   = ProteinDataset(X_train, Y_sparse, val_idx, global_mean, global_std, 0.0, False)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\nval_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n\n# ---------------------------\n# MODEL\n# ---------------------------\nclass MLP(nn.Module):\n    def __init__(self, input_dim, output_dim, hidden=[1024, 512], dropout=0.3):\n        super().__init__()\n        layers = []\n        in_dim = input_dim\n        for h in hidden:\n            layers.append(nn.Linear(in_dim, h))\n            layers.append(nn.LayerNorm(h))\n            layers.append(nn.GELU())\n            layers.append(nn.Dropout(dropout))\n            in_dim = h\n        layers.append(nn.Linear(in_dim, output_dim))\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n\nmodel = MLP(X_train.shape[1], Y_sparse.shape[1]).to(DEVICE)\n\n# ---------------------------\n# LOSS & OPTIMIZER\n# ---------------------------\ndef bce_label_smooth(logits, targets, pos_weight=None, eps=LABEL_SMOOTH):\n    smooth_pos = 1.0 - eps\n    smooth_neg = eps * 0.5\n    targets_sm = targets * smooth_pos + (1 - targets) * smooth_neg\n    criterion = nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n    loss = criterion(logits, targets_sm)\n    return loss.mean()\n\noptimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\nscheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LR, epochs=EPOCHS, steps_per_epoch=len(train_loader))\nscaler = torch.cuda.amp.GradScaler()\n\n# ---------------------------\n# METRICS\n# ---------------------------\ndef f1_weighted_batch(y_true, y_pred_bin, idx_labels, IA_vec):\n    y_true_sub = y_true[:, idx_labels]\n    y_pred_sub = y_pred_bin[:, idx_labels]\n    weights = IA_vec[idx_labels]\n    w_precision_list, w_recall_list = [], []\n\n    for i in range(y_true_sub.shape[0]):\n        true_terms = y_true_sub[i] == 1\n        pred_terms = y_pred_sub[i] == 1\n        if pred_terms.sum() > 0:\n            TP_w = weights[pred_terms & true_terms].sum()\n            Pred_w = weights[pred_terms].sum()\n            w_precision_list.append(TP_w / (Pred_w + 1e-9))\n        True_w = weights[true_terms].sum()\n        if True_w > 0:\n            TP_w = weights[pred_terms & true_terms].sum()\n            w_recall_list.append(TP_w / (True_w + 1e-9))\n    wpr = np.mean(w_precision_list) if w_precision_list else 0.0\n    wrc = np.mean(w_recall_list) if w_recall_list else 0.0\n    return 2 * wpr * wrc / (wpr + wrc + 1e-9) if (wpr + wrc) > 0 else 0.0\n\n@torch.no_grad()\ndef eval_model(model, loader, threshold=0.5, pos_weight=None):\n    F1_MF_list, F1_CC_list, F1_BP_list = [], [], []\n    total_loss = 0.0\n    n_samples = 0\n    model.eval()\n    for xb, yb in loader:\n        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n        logits = model(xb)\n        loss = bce_label_smooth(logits, yb, pos_weight)\n        total_loss += float(loss.item()) * xb.size(0)\n        n_samples += xb.size(0)\n        probs = torch.sigmoid(logits)\n        y_pred_bin = (probs.cpu().numpy() >= threshold).astype(np.float32)\n        y_true = yb.cpu().numpy().astype(np.float32)\n        F1_MF_list.append(f1_weighted_batch(y_true, y_pred_bin, idx_MF, IA_vec))\n        F1_CC_list.append(f1_weighted_batch(y_true, y_pred_bin, idx_CC, IA_vec))\n        F1_BP_list.append(f1_weighted_batch(y_true, y_pred_bin, idx_BP, IA_vec))\n    val_loss = total_loss / n_samples\n    F1_MF = np.mean(F1_MF_list)\n    F1_CC = np.mean(F1_CC_list)\n    F1_BP = np.mean(F1_BP_list)\n    F1_avg = (F1_MF + F1_CC + F1_BP) / 3\n    return val_loss, F1_MF, F1_CC, F1_BP, F1_avg\n\n# ---------------------------\n# TRAIN LOOP\n# ---------------------------\nbest_val_f1 = -1\nbest_val_loss = float('inf')\nbest_threshold = 0.2\n\ntrain_loss_history = []\nval_loss_history = [] \nval_f1_history = []\n\nfor epoch in range(1, EPOCHS+1):\n    t0 = time.time()\n    model.train()\n    total_loss = 0\n    n_samples = 0\n    for xb, yb in train_loader:\n        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n        optimizer.zero_grad()\n        with torch.cuda.amp.autocast():\n            logits = model(xb)\n            loss = bce_label_smooth(logits, yb, pos_weight)\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n        total_loss += float(loss.item()) * xb.size(0)\n        n_samples += xb.size(0)\n    train_loss = total_loss / n_samples\n\n    val_loss, F1_MF, F1_CC, F1_BP, F1_val = eval_model(model, val_loader, threshold=best_threshold, pos_weight=pos_weight)\n    train_loss_history.append(train_loss)\n    val_loss_history.append(val_loss)\n    val_f1_history.append(F1_val)\n    print(f\"[Epoch {epoch}] train_loss={train_loss:.6f} | val_loss={val_loss:.6f} | val_F1={F1_val:.6f} (MF={F1_MF:.6f} CC={F1_CC:.6f} BP={F1_BP:.6f}) | time={time.time()-t0:.1f}s\")\n\n    # save best\n    if F1_val > best_val_f1:\n        best_val_f1 = F1_val\n        torch.save({\n            \"model_state\": model.state_dict(),\n            \"best_threshold\": best_threshold\n        }, OUT_MODEL)\n        print(\" -> saved best model\")\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save({\n            \"model_state\": model.state_dict(),\n            \"best_threshold\": best_threshold\n        }, OUT_MODEL2)\n        print(\" -> saved best model (Val Loss)\")\n\n\n\n# ---------------------------\nepochs = range(1, EPOCHS+1)\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12,5))\n\n# Loss\nplt.subplot(1,2,1)\nplt.plot(epochs, train_loss_history, label='Train Loss')\nplt.plot(epochs, val_loss_history, label='Val Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Train vs Val Loss')\nplt.legend()\nplt.grid(True)\n\n# F1\nplt.subplot(1,2,2)\nplt.plot(epochs, val_f1_history, label='Val F1 Weighted', color='green')\nplt.xlabel('Epoch')\nplt.ylabel('F1 Weighted')\nplt.title('Validation F1 Weighted')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n# ---------------------------\n# PREDICTION\n# ---------------------------\nprint(\"Predicting test set no 1...\")\nckpt = torch.load(OUT_MODEL, map_location=DEVICE)\nmodel.load_state_dict(ckpt[\"model_state\"])\nmodel.eval()\nbest_threshold = ckpt[\"best_threshold\"]\n\nwith open(OUT_SUBMIT, \"w\", newline=\"\") as f:\n    writer = csv.writer(f, delimiter=\"\\t\")\n    writer.writerow([\"ID\", \"GO_ID\", \"score\"])\n\n    test_loader = DataLoader(ProteinDataset(X_test, None, mean=global_mean, std=global_std, train=False),\n                             batch_size=128, shuffle=False)\n    for i, xb in enumerate(test_loader):\n        xb = xb.to(DEVICE)\n        with torch.no_grad():\n            logits = model(xb)\n            probs = torch.sigmoid(logits).cpu().numpy()\n            probs_rescore = probs  # Chỉ dùng xác suất gốc\n    \n        for j in range(probs.shape[0]):\n            pid = ids_test[i*128 + j]\n            row_rescore = probs_rescore[j]\n            topk_idx = np.argsort(row_rescore)[::-1][:TOP_K]\n            for idx in topk_idx:\n                score = float(probs[j, idx])\n                if score > 0.0:\n                    writer.writerow([pid, GO_terms[idx], score])\n\nprint(\"Predicting test set no 2...\")\nckpt = torch.load(OUT_MODEL2, map_location=DEVICE)\nmodel.load_state_dict(ckpt[\"model_state\"])\nmodel.eval()\nbest_threshold = ckpt[\"best_threshold\"]\n\nwith open(OUT_SUBMIT2, \"w\", newline=\"\") as f:\n    writer = csv.writer(f, delimiter=\"\\t\")\n    writer.writerow([\"ID\", \"GO_ID\", \"score\"])\n\n    test_loader = DataLoader(ProteinDataset(X_test, None, mean=global_mean, std=global_std, train=False),\n                             batch_size=128, shuffle=False)\n    for i, xb in enumerate(test_loader):\n        xb = xb.to(DEVICE)\n        with torch.no_grad():\n            logits = model(xb)\n            probs = torch.sigmoid(logits).cpu().numpy()\n            probs_rescore = probs  # Chỉ dùng xác suất gốc\n    \n        for j in range(probs.shape[0]):\n            pid = ids_test[i*128 + j]\n            row_rescore = probs_rescore[j]\n            topk_idx = np.argsort(row_rescore)[::-1][:TOP_K]\n            for idx in topk_idx:\n                score = float(probs[j, idx])\n                if score > 0.0:\n                    writer.writerow([pid, GO_terms[idx], score])\n\nprint(\"All done.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}