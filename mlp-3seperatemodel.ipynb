{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14021088,"sourceType":"datasetVersion","datasetId":8930262},{"sourceId":14030877,"sourceType":"datasetVersion","datasetId":8934740},{"sourceId":14034747,"sourceType":"datasetVersion","datasetId":8937311},{"sourceId":14082997,"sourceType":"datasetVersion","datasetId":8965968},{"sourceId":14094271,"sourceType":"datasetVersion","datasetId":8975090},{"sourceId":14095477,"sourceType":"datasetVersion","datasetId":8976080},{"sourceId":14104741,"sourceType":"datasetVersion","datasetId":8983454},{"sourceId":678387,"sourceType":"modelInstanceVersion","modelInstanceId":514502,"modelId":529149}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**1. TRAIN CHUNG 1 MODEL, LẤY TOP-K, NHƯ DƯỚI LÀ TOP-K = 150**","metadata":{}},{"cell_type":"markdown","source":"**2. TRAIN TÁCH BIỆT 3 MODEL, LẤY TOP C, F, P. NHƯ DƯỚI LÀ LẤY 20-40-145**","metadata":{}},{"cell_type":"code","source":"# deepgo_pipeline_split_aspect.py\nimport os\nimport time\nimport csv\nimport numpy as np\nfrom scipy.sparse import load_npz\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport matplotlib.pyplot as plt\n\n# ---------------------------\n# CONFIG\n# ---------------------------\nSEED = 42\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 16\nEPOCHS = 40\nLR = 5e-4\nWEIGHT_DECAY = 1e-4\nCLIP_NORM = 5.0\nFEATURE_DROPOUT = 0.2\nLABEL_SMOOTH = 0.03\nTHRESHOLD = 0.2\n\nTOP_K = {\"C\": 20, \"F\": 50, \"P\": 175}\nOUT_SUBMIT_FINAL = \"/kaggle/working/submission.tsv\"\n\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# ---------------------------\n# LOAD DATA\n# ---------------------------\nX_train = np.load('/kaggle/input/cafa56-end/650_taxon_features_X_INPUT.npy')\nX_test = np.load('/kaggle/input/cafa56-end/X_test.npy')\nids_test = np.load('/kaggle/input/cafa56-end/protein_ids_test.npy')\n\nY_sparse_C = load_npz(\"/kaggle/input/cafa56-end/Y_C.npz\")\nGO_terms_C = np.load(\"/kaggle/input/cafa56-end/GO_terms_C.npy\", allow_pickle=True)\n\nY_sparse_F = load_npz(\"/kaggle/input/cafa56-end/Y_F.npz\")\nGO_terms_F = np.load(\"/kaggle/input/cafa56-end/GO_terms_F.npy\", allow_pickle=True)\n\nY_sparse_P = load_npz(\"/kaggle/input/cafa56-end/Y_P.npz\")\nGO_terms_P = np.load(\"/kaggle/input/cafa56-end/GO_terms_P.npy\", allow_pickle=True)\n\nY_sparse_dict = {\"C\": Y_sparse_C, \"F\": Y_sparse_F, \"P\": Y_sparse_P}\nGO_terms_dict = {\"C\": GO_terms_C, \"F\": GO_terms_F, \"P\": GO_terms_P}\n\n# normalization\nglobal_mean = X_train.mean(axis=0).astype(np.float32)\nglobal_std  = X_train.std(axis=0).astype(np.float32) + 1e-6\n\n# IA vector\nIA_dict = {}\nwith open(\"/kaggle/input/cafa56-end/IA.tsv\") as f:\n    for line in f:\n        go, value = line.strip().split(\"\\t\")\n        IA_dict[go] = float(value)\nIA_vec_C = np.array([IA_dict.get(go, 0.0) for go in GO_terms_C], dtype=np.float32)\nIA_vec_F = np.array([IA_dict.get(go, 0.0) for go in GO_terms_F], dtype=np.float32)\nIA_vec_P = np.array([IA_dict.get(go, 0.0) for go in GO_terms_P], dtype=np.float32)\n\n# ontology mapping\ngo2asp = {}\nwith open(\"/kaggle/input/mapping-wf1/go_to_aspect.tsv\") as f:\n    next(f)\n    for line in f:\n        go, asp = line.strip().split(\",\")\n        go2asp[go] = asp\n\n# ---------------------------\n# DATASET\n# ---------------------------\nclass ProteinDataset(Dataset):\n    def __init__(self, X, Y_sparse=None, indices=None, mean=None, std=None, feature_dropout=0.0, train=True):\n        self.X = X\n        self.Y = Y_sparse\n        self.indices = np.array(indices) if indices is not None else np.arange(X.shape[0])\n        self.mean = mean\n        self.std = std\n        self.feature_dropout = feature_dropout\n        self.train = train\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, idx):\n        i = int(self.indices[idx])\n        x = self.X[i].astype(np.float32)\n        if self.mean is not None and self.std is not None:\n            x = (x - self.mean) / self.std\n        if self.train and self.feature_dropout > 0.0 and np.random.rand() < 0.5:\n            mask = (np.random.rand(x.shape[0]) >= self.feature_dropout).astype(np.float32)\n            x = x * mask\n\n        x = torch.from_numpy(x)\n\n        if self.Y is not None:\n            y = torch.from_numpy(self.Y[i].toarray().squeeze().astype(np.float32))\n            return x, y\n        else:\n            return x\n\n# ---------------------------\n# MODEL\n# ---------------------------\nclass MLP(nn.Module):\n    def __init__(self, input_dim, output_dim, hidden=[1024, 512], dropout=0.3):\n        super().__init__()\n        layers = []\n        in_dim = input_dim\n        for h in hidden:\n            layers.append(nn.Linear(in_dim, h))\n            layers.append(nn.LayerNorm(h))\n            layers.append(nn.GELU())\n            layers.append(nn.Dropout(dropout))\n            in_dim = h\n        layers.append(nn.Linear(in_dim, output_dim))\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n\n# ---------------------------\n# LOSS\n# ---------------------------\ndef bce_label_smooth(logits, targets, pos_weight=None, eps=LABEL_SMOOTH):\n    smooth_pos = 1.0 - eps\n    smooth_neg = eps * 0.5\n    targets_sm = targets * smooth_pos + (1 - targets) * smooth_neg\n    criterion = nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n    loss = criterion(logits, targets_sm)\n    return loss.mean()\n\n# ---------------------------\n# dummy eval (giữ nguyên logic gốc)\n# ---------------------------\n@torch.no_grad()\ndef f1_weighted_batch(y_true, y_pred_bin, IA_vec):\n    weights = IA_vec\n    w_precision_list, w_recall_list = [], []\n\n    for i in range(y_true.shape[0]):\n        true_terms = y_true[i] == 1\n        pred_terms = y_pred_bin[i] == 1\n        if pred_terms.sum() > 0:\n            TP_w = weights[pred_terms & true_terms].sum()\n            Pred_w = weights[pred_terms].sum()\n            w_precision_list.append(TP_w / (Pred_w + 1e-9))\n        True_w = weights[true_terms].sum()\n        if True_w > 0:\n            TP_w = weights[pred_terms & true_terms].sum()\n            w_recall_list.append(TP_w / (True_w + 1e-9))\n    wpr = np.mean(w_precision_list) if w_precision_list else 0.0\n    wrc = np.mean(w_recall_list) if w_recall_list else 0.0\n    return 2 * wpr * wrc / (wpr + wrc + 1e-9) if (wpr + wrc) > 0 else 0.0\n\ndef eval_model(model, loader, IA_vec, threshold=0.5, pos_weight=None):\n    total_loss = 0.0\n    n_samples = 0\n    model.eval()\n    F1_list = []\n\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n\n            logits = model(xb)\n            loss = bce_label_smooth(logits, yb, pos_weight)\n\n            total_loss += float(loss.item()) * xb.size(0)\n            n_samples += xb.size(0)\n\n            probs = torch.sigmoid(logits)\n            y_pred_bin = (probs.cpu().numpy() >= threshold).astype(np.float32)\n            y_true = yb.cpu().numpy().astype(np.float32)\n\n            F1_list.append(f1_weighted_batch(y_true, y_pred_bin, IA_vec))\n\n    val_loss = total_loss / n_samples\n    val_f1 = float(np.mean(F1_list))\n    return val_loss, val_f1 \n\n\n  \n\n# ---------------------------\n# TRAIN FUNCTION\n# ---------------------------\ndef train_aspect(X_train, Y_sparse_aspect, aspect_name):\n\n\n    Y_sparse_aspect = Y_sparse_aspect.tocsr()\n\n    \n    row_nnz = np.diff(Y_sparse_aspect.indptr)\n    valid_idx = np.where(row_nnz > 0)[0]\n    train_idx, val_idx = train_test_split(valid_idx, test_size=0.1, random_state=SEED)\n\n    train_ds = ProteinDataset(X_train, Y_sparse_aspect, train_idx, global_mean, global_std, FEATURE_DROPOUT, True)\n    val_ds   = ProteinDataset(X_train, Y_sparse_aspect, val_idx, global_mean, global_std, 0.0, False)\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n    val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n\n    model = MLP(X_train.shape[1], Y_sparse_aspect.shape[1]).to(DEVICE)\n\n    train_sparse = Y_sparse_aspect[train_idx]\n    label_freq = np.array(train_sparse.sum(axis=0)).squeeze()\n    N_train = len(train_idx)\n    pos_weight_arr = (N_train - label_freq) / (label_freq + 1e-8)\n    pos_weight_arr = np.clip(pos_weight_arr, 1.0, 5.0)\n    pos_weight = torch.tensor(pos_weight_arr, dtype=torch.float32).to(DEVICE)\n\n    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LR, epochs=EPOCHS,\n                                              steps_per_epoch=len(train_loader))\n    scaler = torch.cuda.amp.GradScaler()\n\n    best_val_f1 = -1\n    OUT_MODEL = f\"/kaggle/working/best_model_{aspect_name}.pt\"\n\n    # ---- NEW HISTORY ----\n    hist_train_loss = []\n    hist_val_loss = []\n    hist_val_f1 = []\n\n    for epoch in range(1, EPOCHS+1):\n        model.train()\n        total_loss = 0\n        n_samples = 0\n\n        for xb, yb in train_loader:\n            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n            optimizer.zero_grad()\n\n            with torch.cuda.amp.autocast():\n                logits = model(xb)\n                loss = bce_label_smooth(logits, yb, pos_weight)\n\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n\n            total_loss += float(loss.item()) * xb.size(0)\n            n_samples += xb.size(0)\n\n        train_loss = total_loss / n_samples\n\n\n        IA_vec_map = {\n            'C': IA_vec_C,\n            'F': IA_vec_F,\n            'P': IA_vec_P\n        }\n\n        # Lấy giá trị (có thể thêm default=None nếu cần)\n        IA_vec = IA_vec_map.get(aspect_name)\n        \n        val_loss, val_f1 = eval_model(model, val_loader, IA_vec, threshold=0.5, pos_weight=pos_weight)\n\n        print(f\"{aspect_name} Epoch {epoch}: train_loss={train_loss:.6f} val_loss={val_loss:.6f} val_F1={val_f1:.6f}\")\n\n        # save history\n        hist_train_loss.append(train_loss)\n        hist_val_loss.append(val_loss)\n        hist_val_f1.append(val_f1)\n\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            torch.save({\"model_state\": model.state_dict()}, OUT_MODEL)\n            print(f\" -> saved best model {aspect_name}\")\n\n    return OUT_MODEL, hist_train_loss, hist_val_loss, hist_val_f1\n\n# ---------------------------\n# TRAIN 3 aspects + SAVE history\n# ---------------------------\ntrained_models = {}\nhistory = {}\n\nfor asp in [\"C\",\"F\",\"P\"]:\n    print(f\"Training aspect {asp} ...\")\n    model_path, tr_l, vl_l, vf1 = train_aspect(X_train, Y_sparse_dict[asp], asp)\n    trained_models[asp] = model_path\n    history[asp] = {\n        \"train_loss\": tr_l,\n        \"val_loss\": vl_l,\n        \"val_f1\": vf1\n    }\n\n# ---------------------------\n# PLOT TRAINING CURVES\n# ---------------------------\nfor asp in [\"C\",\"F\",\"P\"]:\n    h = history[asp]\n    epochs = range(1, len(h[\"train_loss\"]) + 1)\n\n    plt.figure(figsize=(12, 4))\n\n    # Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, h[\"train_loss\"], label=\"Train Loss\")\n    plt.plot(epochs, h[\"val_loss\"], label=\"Val Loss\")\n    plt.title(f\"{asp} – Loss Curve\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n\n    # F1\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, h[\"val_f1\"], label=\"Val F1\", color=\"green\")\n    plt.title(f\"{asp} – Validation F1\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"F1 Score\")\n    plt.legend()\n\n    plt.tight_layout()\n    plt.savefig(f\"/kaggle/working/{asp}_training_plots.png\")\n    plt.show()\n\nprint(\"Training curves saved in /kaggle/working/\")\n\n# ---------------------------\n# PREDICT AND MERGE\n# ---------------------------\ntest_ds = ProteinDataset(X_test, None, mean=global_mean, std=global_std, train=False)\ntest_loader = DataLoader(test_ds, batch_size=128, shuffle=False)\n\nwith open(OUT_SUBMIT_FINAL, \"w\", newline=\"\") as f:\n    writer = csv.writer(f, delimiter=\"\\t\")\n    writer.writerow([\"ID\", \"GO_ID\", \"score\"])\n\n    for asp in [\"C\",\"F\",\"P\"]:\n        print(f\"Predicting aspect {asp} ...\")\n        model_path = trained_models[asp]\n        GO_terms_aspect = GO_terms_dict[asp]\n        top_k = TOP_K[asp]\n\n        ckpt = torch.load(model_path, map_location=DEVICE)\n        model = MLP(X_train.shape[1], len(GO_terms_aspect)).to(DEVICE)\n        model.load_state_dict(ckpt[\"model_state\"])\n        model.eval()\n\n        for i, xb in enumerate(test_loader):\n            xb = xb.to(DEVICE)\n            with torch.no_grad():\n                logits = model(xb)\n                probs = torch.sigmoid(logits).cpu().numpy()\n\n            for j in range(probs.shape[0]):\n                pid = ids_test[i*128 + j]\n                row_rescore = probs[j]\n                topk_idx = np.argsort(row_rescore)[::-1][:top_k]\n                for idx in topk_idx:\n                    score = float(probs[j, idx])\n                    if score > 0.0:\n                        writer.writerow([pid, GO_terms_aspect[idx], score])\n\nprint(\"All done. Submission file:\", OUT_SUBMIT_FINAL)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3. CÁI NÀY CHỈ LẤY TOP K TỪ MODEL ĐÃ TRAIN**","metadata":{}},{"cell_type":"code","source":"# import os\n# import time\n# import csv\n# import numpy as np\n# from scipy.sparse import load_npz\n# from sklearn.model_selection import train_test_split\n\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.utils.data import Dataset, DataLoader\n\n# import matplotlib.pyplot as plt\n\n# # ---------------------------\n# # CONFIG\n# # ---------------------------\n# SEED = 42\n# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# BATCH_SIZE = 16\n# EPOCHS = 40\n# LR = 5e-4\n# WEIGHT_DECAY = 1e-4\n# CLIP_NORM = 5.0\n# FEATURE_DROPOUT = 0.2\n# LABEL_SMOOTH = 0.03\n# THRESHOLD = 0.2\n\n# TOP_K = {\"C\": 25, \"F\": 75, \"P\": 200}\n# OUT_SUBMIT_FINAL = \"/kaggle/working/submission.tsv\"\n\n# np.random.seed(SEED)\n# torch.manual_seed(SEED)\n# if torch.cuda.is_available():\n#     torch.cuda.manual_seed_all(SEED)\n\n# # ---------------------------\n# # LOAD DATA\n# # ---------------------------\n# X_train = np.load('/kaggle/input/cafa-m56/CAFA56/cafa56_esm2_taxon_features_X.npy')\n# X_test = np.load('/kaggle/input/cafa-m56/CAFA56/cafa56_X_test.npy')\n# ids_test = np.load('/kaggle/input/cafa-m56/CAFA56/cafa56_protein_ids_test.npy')\n\n# Y_sparse_C = load_npz(\"/kaggle/input/cafa56-splitfunc/Y_C.npz\")\n# GO_terms_C = np.load(\"/kaggle/input/cafa56-splitfunc/GO_terms_C.npy\", allow_pickle=True)\n\n# Y_sparse_F = load_npz(\"/kaggle/input/cafa56-splitfunc/Y_F.npz\")\n# GO_terms_F = np.load(\"/kaggle/input/cafa56-splitfunc/GO_terms_F.npy\", allow_pickle=True)\n\n# Y_sparse_P = load_npz(\"/kaggle/input/cafa56-splitfunc/Y_P.npz\")\n# GO_terms_P = np.load(\"/kaggle/input/cafa56-splitfunc/GO_terms_P.npy\", allow_pickle=True)\n\n# Y_sparse_dict = {\"C\": Y_sparse_C, \"F\": Y_sparse_F, \"P\": Y_sparse_P}\n# GO_terms_dict = {\"C\": GO_terms_C, \"F\": GO_terms_F, \"P\": GO_terms_P}\n\n# # normalization\n# global_mean = X_train.mean(axis=0).astype(np.float32)\n# global_std  = X_train.std(axis=0).astype(np.float32) + 1e-6\n\n# # IA vector\n# IA_dict = {}\n# with open(\"/kaggle/input/cafa6-data/IA.tsv\") as f:\n#     for line in f:\n#         go, value = line.strip().split(\"\\t\")\n#         IA_dict[go] = float(value)\n# IA_vec_C = np.array([IA_dict.get(go, 0.0) for go in GO_terms_C], dtype=np.float32)\n# IA_vec_F = np.array([IA_dict.get(go, 0.0) for go in GO_terms_F], dtype=np.float32)\n# IA_vec_P = np.array([IA_dict.get(go, 0.0) for go in GO_terms_P], dtype=np.float32)\n\n# # ontology mapping\n# go2asp = {}\n# with open(\"/kaggle/input/mapping-wf1/go_to_aspect.tsv\") as f:\n#     next(f)\n#     for line in f:\n#         go, asp = line.strip().split(\",\")\n#         go2asp[go] = asp\n\n# class ProteinDataset(Dataset):\n#     def __init__(self, X, Y_sparse=None, indices=None, mean=None, std=None, feature_dropout=0.0, train=True):\n#         self.X = X\n#         self.Y = Y_sparse\n#         self.indices = np.array(indices) if indices is not None else np.arange(X.shape[0])\n#         self.mean = mean\n#         self.std = std\n#         self.feature_dropout = feature_dropout\n#         self.train = train\n\n#     def __len__(self):\n#         return len(self.indices)\n\n#     def __getitem__(self, idx):\n#         i = int(self.indices[idx])\n#         x = self.X[i].astype(np.float32)\n#         if self.mean is not None and self.std is not None:\n#             x = (x - self.mean) / self.std\n#         if self.train and self.feature_dropout > 0.0 and np.random.rand() < 0.5:\n#             mask = (np.random.rand(x.shape[0]) >= self.feature_dropout).astype(np.float32)\n#             x = x * mask\n\n#         x = torch.from_numpy(x)\n\n#         if self.Y is not None:\n#             y = torch.from_numpy(self.Y[i].toarray().squeeze().astype(np.float32))\n#             return x, y\n#         else:\n#             return x\n\n# class DeepGO_MLP(nn.Module):\n#     def __init__(self, input_dim, output_dim, hidden=[1024, 512], dropout=0.3):\n#         super().__init__()\n#         layers = []\n#         in_dim = input_dim\n#         for h in hidden:\n#             layers.append(nn.Linear(in_dim, h))\n#             layers.append(nn.LayerNorm(h))\n#             layers.append(nn.GELU())\n#             layers.append(nn.Dropout(dropout))\n#             in_dim = h\n#         layers.append(nn.Linear(in_dim, output_dim))\n#         self.net = nn.Sequential(*layers)\n\n#     def forward(self, x):\n#         return self.net(x)\n            \n# test_ds = ProteinDataset(X_test, None, mean=global_mean, std=global_std, train=False)\n# test_loader = DataLoader(test_ds, batch_size=128, shuffle=False)\n\n# trained_models = {}\n# trained_models[\"C\"] = \"/kaggle/input/3splitfunc/other/default/1/best_model_C.pt\"\n# trained_models[\"F\"] = \"/kaggle/input/3splitfunc/other/default/1/best_model_F.pt\"\n# trained_models[\"P\"] = \"/kaggle/input/3splitfunc/other/default/1/best_model_P.pt\"\n\n# with open(OUT_SUBMIT_FINAL, \"w\", newline=\"\") as f:\n#     writer = csv.writer(f, delimiter=\"\\t\")\n#     writer.writerow([\"ID\", \"GO_ID\", \"score\"])\n\n#     for asp in [\"C\",\"F\",\"P\"]:\n#         print(f\"Predicting aspect {asp} ...\")\n#         model_path = trained_models[asp]\n#         GO_terms_aspect = GO_terms_dict[asp]\n#         top_k = TOP_K[asp]\n\n#         ckpt = torch.load(model_path, map_location=DEVICE)\n#         model = DeepGO_MLP(X_train.shape[1], len(GO_terms_aspect)).to(DEVICE)\n#         model.load_state_dict(ckpt[\"model_state\"])\n#         model.eval()\n\n#         for i, xb in enumerate(test_loader):\n#             xb = xb.to(DEVICE)\n#             with torch.no_grad():\n#                 logits = model(xb)\n#                 probs = torch.sigmoid(logits).cpu().numpy()\n\n#             for j in range(probs.shape[0]):\n#                 pid = ids_test[i*128 + j]\n#                 row_rescore = probs[j]\n#                 topk_idx = np.argsort(row_rescore)[::-1][:top_k]\n#                 for idx in topk_idx:\n#                     score = float(probs[j, idx])\n#                     if score > 0.0:\n#                         writer.writerow([pid, GO_terms_aspect[idx], score])\n\n\n# import pandas as pd\n\n# print(\"------------------------------------------------------\")\n# print(f\"Post-processing: Sorting {OUT_SUBMIT_FINAL} by ID...\")\n\n# # 1. Đọc file TSV lên bằng Pandas\n# # Lưu ý: header=None vì code trước đó thường không ghi header cho CAFA. \n# # Nếu code của bạn CÓ ghi header [\"ID\", \"GO_ID\", \"score\"], hãy đổi thành header=0\n# df = pd.read_csv(OUT_SUBMIT_FINAL, sep='\\t', header=None, names=[\"ID\", \"GO_ID\", \"score\"])\n\n# # 2. Sắp xếp theo cột ID\n# df = df.sort_values(by=\"ID\", ascending=True)\n\n# # 3. Ghi đè lại file cũ\n# # header=False để không ghi tên cột vào file output\n# df.to_csv(OUT_SUBMIT_FINAL, sep='\\t', index=False, header=False)\n\n# print(f\"Sorting complete. Final file saved to {OUT_SUBMIT_FINAL}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T01:47:05.349079Z","iopub.execute_input":"2025-12-10T01:47:05.349361Z","iopub.status.idle":"2025-12-10T01:56:12.713249Z","shell.execute_reply.started":"2025-12-10T01:47:05.349335Z","shell.execute_reply":"2025-12-10T01:56:12.712062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}